%=============================================================================
\subsection{Routing \& Relevance Stack}
\label{sec:routing-stack}

\textbf{Stack overview:} This stack determines which parts of the input are relevant to the current task and routes attention accordingly. These heads filter information, focus on salient content, and manage global context.

%-----------------------------------------------------------------------------
\subsubsection{(M) Topic-Relevance Heads}
\label{head:topic-relevance}

\noindent\depthinfo{0.35--0.60} | \litnames{topic-relevance head, relevance head, topic head, salience head, filter head, subject head, domain head}

\begin{functiondesc}
Identify the primary topic or subject matter and determine which parts of the input context are relevant to the current generation task. These heads filter out irrelevant information while highlighting salient content that should influence output. They operate by computing relevance scores based on semantic similarity, task alignment, and topical coherence. These heads maintain topic coherence across generation by attending to topic-establishing phrases and domain indicators. Important for handling long contexts where most information may not be pertinent to the immediate query, they work early enough to guide downstream attention but late enough to understand task requirements. These heads reduce noise, improve focus on task-relevant material, promote on-topic responses, and maintain thematic consistency.
\end{functiondesc}

\begin{attentionbox}
\attstrong{Task-relevant content, query-related information, topically aligned tokens, topic indicators, subject headings, domain markers, thematic keywords}\\
\attweak{Off-topic material, tangential content, unrelated context, function words, generic content, structural tokens}\\
\attreacts{Semantic relevance, topical alignment, task-content matching, topic transitions, subject establishment, domain signals}
\end{attentionbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate reduction in focus on relevant information with increased topic drift. Model more easily distracted by irrelevant content. Longer contexts show greater impact. May include off-topic information in responses or miss key relevant details. Responses may wander off-topic or fail to maintain consistent subject focus. Multi-turn conversations show more topic inconsistency. Domain-specific framing becomes less reliable.
\end{ablationbox}

\begin{examplebox}
\exinput{``[Long document about cars, climate, and history] What caused the 2008 financial crisis? Let's discuss quantum entanglement. How does it relate to...''}\\
\exbehavior{Attend to query, mark financial/economic content as relevant, de-emphasize cars/climate; identify ``quantum entanglement'' as primary topic, maintain physics domain framing}\\
\exeffect{Response focuses on pertinent economic information, ignoring unrelated context; subsequent responses stay within quantum physics domain rather than drifting to unrelated topics}
\end{examplebox}

\headfooter{\statuswell}{focus (L), router (L), entity (M)}

%-----------------------------------------------------------------------------
\subsubsection{(L) Focus Heads}
\label{head:focus}

\noindent\depthinfo{0.65--0.80} | \litnames{focus head, attention-routing head, spotlight head}

\begin{functiondesc}
Concentrate attention on the most salient elements for the current generation step. These heads implement dynamic focus allocation by suppressing less important content and amplifying critical information. More selective than topic-relevance heads, they operate at higher specificity to determine exactly which tokens should influence the next token prediction. These heads shift focus as generation proceeds, moving attention between different aspects of the context. Important for maintaining coherent narrative flow and ensuring responses address the most important aspects of queries.
\end{functiondesc}

\begin{attentionbox}
\attstrong{Currently salient tokens, query-critical content, immediate context for next token}\\
\attweak{Background information, previously-processed content, low-priority details}\\
\attreacts{Query emphasis, current generation needs, token-specific relevance}
\end{attentionbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate reduction in focus precision and less targeted responses. Model may give equal weight to important and peripheral information. Answers become more diffuse, less direct. Reduced ability to prioritize key information in complex contexts.
\end{ablationbox}

\begin{examplebox}
\exinput{``Among all these details, what is the MAIN cause of the problem?''}\\
\exbehavior{Attend strongly to ``MAIN cause'', focus on causal information, suppress secondary details}\\
\exeffect{Response directly addresses primary cause rather than listing all contributing factors}
\end{examplebox}

\headfooter{\statuswell}{topic-relevance (M), router (L)}

%-----------------------------------------------------------------------------
\subsubsection{(L) Router Heads}
\label{head:router}

\noindent\depthinfo{0.70--0.85} | \litnames{router head, dispatch head, task-routing head}

\begin{functiondesc}
Route different types of queries to appropriate processing strategies or knowledge domains. These heads act as dispatchers that recognize query type (factual, creative, analytical, procedural) and bias processing toward suitable approaches. They activate different downstream heads based on task classification. Similar to mixture-of-experts routing but at the attention level, these heads are important for multi-capability models that need to handle diverse query types with different processing requirements. They enable dynamic strategy selection based on input characteristics.
\end{functiondesc}

\begin{attentionbox}
\attstrong{Query-type indicators, task markers, domain signals, instruction verbs}\\
\attweak{Content details, specific entities, output tokens}\\
\attreacts{Task classification cues, query structure, capability requirements}
\end{attentionbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate reduction in task-appropriate processing with suboptimal strategy selection. Model may use creative approaches for factual queries or analytical methods for creative tasks. Reduced specialization in handling different query types.
\end{ablationbox}

\begin{examplebox}
\exinput{``Calculate the compound interest vs. Write a poem about compound interest''}\\
\exbehavior{Route first to mathematical processing, second to creative generation}\\
\exeffect{Appropriate strategy activation: calculation for first, literary devices for second}
\end{examplebox}

\headfooter{\statusobs}{focus (L), mode-switch (M), instruction (E)}

%-----------------------------------------------------------------------------
\subsubsection{(F) Global-Attention Heads}
\label{head:global-attention}

\noindent\depthinfo{0.88--0.96} | \litnames{global-attention head, full-context head, summary-attention head}

\begin{functiondesc}
Maintain broad attention over the entire context to integrate global information in final generation stages. Unlike focused or selective attention heads, these heads attend widely to ensure the complete picture is considered before output finalization. Particularly important for coherence checking, ensuring responses account for all relevant context, and preventing local optimization at the expense of global consistency, these heads catch context elements that earlier focused attention might have missed. They act as a final integration mechanism.
\end{functiondesc}

\begin{attentionbox}
\attstrong{All context tokens, document-level information, global constraints}\\
\attweak{Fine-grained local patterns, individual token details}\\
\attreacts{Complete context, document-level coherence, global consistency requirements}
\end{attentionbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate reduction in global coherence with increased context inconsistencies. Responses may miss relevant information from distant parts of context. More locally optimal but globally suboptimal outputs. Coherence issues in long-context scenarios.
\end{ablationbox}

\begin{examplebox}
\exinput{[Long context with constraint mentioned early: ``Keep it under 100 words'']}\\
\exbehavior{Maintain attention on length constraint throughout generation}\\
\exeffect{Final response respects word limit despite constraint appearing far from generation point}
\end{examplebox}

\headfooter{\statusobs}{focus (L), topic-relevance (M), completion-stabilization (F)}

%-----------------------------------------------------------------------------
\subsubsection{(F) Implicit-RAG Routing Heads}
\label{head:implicit-rag}

\noindent\depthinfo{0.90--0.98} | \litnames{implicit-RAG head, knowledge-routing head, retrieval-simulation head, rag-routing head}

\begin{functiondesc}
Route attention to knowledge-bearing portions of the context in a way that mimics retrieval-augmented generation (RAG) patterns, even without explicit retrieval mechanisms. These heads identify and prioritize factual, knowledge-dense segments that should ground the response. They recognize quoted material, factual statements, and authoritative information sources within context. Acting as an implicit retrieval mechanism by selectively attending to information that should be treated as retrieved knowledge, these heads are important for grounding responses in provided context rather than pure generation.
\end{functiondesc}

\begin{attentionbox}
\attstrong{Factual statements, quoted material, authoritative sources, knowledge-dense segments}\\
\attweak{Opinions, questions, purely conversational elements}\\
\attreacts{Citation markers, factual density, authoritative tone, structured information}
\end{attentionbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate decrease in context utilization with reduced grounding in provided context. Model more likely to rely on parametric knowledge rather than provided information. Less effective use of quoted material or reference content. Responses less anchored to specific context.
\end{ablationbox}

\begin{examplebox}
\exinput{``According to the document: `GDP grew 3.2\% in Q3.' What was the growth rate?''}\\
\exbehavior{Strongly attend to quoted factual content, treat as authoritative source}\\
\exeffect{Response grounds answer in provided data: ``3.2\%'' rather than hallucinating different figure}
\end{examplebox}

\headfooter{\statusobs}{global-attention (F), fact (M)}
